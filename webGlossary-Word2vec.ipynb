{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flask --user\n",
    "def func(dataset):\n",
    "    import pandas as pd\n",
    "#     dataset=pd.read_csv(requirement)\n",
    "    CRE_corpus=dataset['feature']+','+dataset['benefit']+'.'\n",
    "    from urllib.request import urlopen\n",
    "    url=\"https://en.wikipedia.org/wiki/Category:Home_automation\"\n",
    "    data=urlopen(url)\n",
    "    html=data.read()\n",
    "    # print(html)\n",
    "    data.close()\n",
    "    from bs4 import BeautifulSoup as soup\n",
    "    html_soup=soup(html,'html.parser')\n",
    "    subcat=html_soup.findAll('div',{'id':'mw-subcategories'})\n",
    "    list=subcat[0].findAll('li')\n",
    "    HA_corpus=[]\n",
    "    for i in range(0,len(list)):\n",
    "        link=list[i].find('a')\n",
    "        url1=\"https://en.wikipedia.org\"+link['href']\n",
    "        data1=urlopen(url1)\n",
    "        html1=data1.read()\n",
    "    #     print(html1)\n",
    "        data1.close()\n",
    "        html_soup1=soup(html1,'html.parser')\n",
    "        subcat3=html_soup1.findAll('div',{'id':'mw-subcategories'})\n",
    "        if len(subcat3)>0:\n",
    "            list3=subcat3[0].findAll('li')\n",
    "            for j in range(0,len(list3)):\n",
    "                link3=list3[j].find('a')\n",
    "                url3=\"https://en.wikipedia.org\"+link3['href']\n",
    "                data3=urlopen(url3)\n",
    "                html3=data3.read()\n",
    "                data3.close()\n",
    "                html_soup3=soup(html3,'html.parser')\n",
    "                cat3=html_soup3.findAll('div',{'id':'mw-pages'})\n",
    "                list4=cat3[0].findAll('li')\n",
    "                for k in range(0,len(list4)):\n",
    "                    text4=list4[k].find('a').text\n",
    "                    HA_corpus.append(text4)\n",
    "        cat1=html_soup1.findAll('div',{'id':'mw-pages'})\n",
    "        list1=cat1[0].findAll('li')\n",
    "        for j in range(0,len(list1)):\n",
    "            text1=list1[j].find('a').text\n",
    "            HA_corpus.append(text1)\n",
    "    # print(HA_corpus)   \n",
    "    cat2=html_soup.findAll('div',{'id':'mw-pages'})\n",
    "    list2=cat2[0].findAll('li')\n",
    "    for i in range(0,len(list2)):\n",
    "        text2=list2[i].find('a').text\n",
    "        HA_corpus.append(text2)\n",
    "    # print(HA_corpus)\n",
    "    # 1. Tokenization\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    HA_tokens=[]\n",
    "    for i in range(0,len(HA_corpus)):\n",
    "      alpha_tokens=[word for word in HA_corpus[i].split() if word.isalpha()]\n",
    "      result=' '.join(alpha_tokens)\n",
    "      tokens=nltk.word_tokenize(result)\n",
    "      for j in range(0,len(tokens)):\n",
    "        tokens[j]=tokens[j].lower()\n",
    "      HA_tokens.append(tokens)\n",
    "    # print(HA_tokens)\n",
    "    CRE_tokens=[]\n",
    "    for i in range(0,len(CRE_corpus)):\n",
    "      alpha_tokens=[word for word in CRE_corpus[i].split() if word.isalpha()]\n",
    "      result=' '.join(alpha_tokens)\n",
    "      tokens=nltk.word_tokenize(result)\n",
    "      for j in range(0,len(tokens)):\n",
    "        tokens[j]=tokens[j].lower()\n",
    "      CRE_tokens.append(tokens)\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('omw-1.4')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    nltk.download('wordnet')\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    new_HA_tokens=[]\n",
    "    for i in range(0,len(HA_tokens)):\n",
    "      temp=[]\n",
    "      for token in HA_tokens[i]:\n",
    "        if token not in stop_words:\n",
    "          temp.append(lemmatizer.lemmatize(token))\n",
    "      new_HA_tokens.append(temp)\n",
    "    # print(new_HA_tokens)\n",
    "    new_CRE_tokens=[]\n",
    "    for i in range(0,len(CRE_tokens)):\n",
    "      temp=[]\n",
    "      for token in CRE_tokens[i]:\n",
    "        if token not in stop_words:\n",
    "          temp.append(lemmatizer.lemmatize(token))\n",
    "      new_CRE_tokens.append(temp)\n",
    "    # print(new_CRE_tokens)\n",
    "    #pos_tagging\n",
    "    from nltk.tag import pos_tag\n",
    "    CRE=[]\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    for i in range(0,len(new_CRE_tokens)):\n",
    "      CRE.append(pos_tag(new_CRE_tokens[i]))\n",
    "    HA=[]\n",
    "    for i in range(0,len(new_HA_tokens)):\n",
    "      HA.append(pos_tag(new_HA_tokens[i]))\n",
    "    # text chunking\n",
    "    import nltk\n",
    "    # nltk.download('maxent_ne_chunker')\n",
    "    nltk.download('words')\n",
    "    from nltk import RegexpParser\n",
    "    grammer = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "#     grammer = \"NP: {<DT>?<JJ>*<NN.*>}\"\n",
    "    cp = nltk.RegexpParser(grammer)\n",
    "    new_CRE=[]\n",
    "    for i in range(0,len(CRE)):\n",
    "      # output=nltk.chunk.ne_chunk(CRE[i])\n",
    "      output = cp.parse(CRE[i])\n",
    "      new_CRE.append(output)\n",
    "    new_HA=[]\n",
    "    for i in range(0,len(HA)):\n",
    "      # output=nltk.chunk.ne_chunk(HA[i])\n",
    "      if(not HA[i]):\n",
    "        continue\n",
    "      output = cp.parse(HA[i])\n",
    "      new_HA.append(output)\n",
    "    #extracting noun phrases from CRE\n",
    "    CRE_NP = []\n",
    "    for i in range(0,len(new_CRE)):\n",
    "      tree = new_CRE[i]\n",
    "      for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "          if(subtree.label()==\"NP\"):\n",
    "            np=[]\n",
    "            for leaf in subtree:\n",
    "              np.append(leaf[0])\n",
    "            # np=np[:-1]\n",
    "            CRE_NP.append(np)\n",
    "    # print(CRE_NP)\n",
    "    #extracting noun phrases from HA\n",
    "    HA_NP = []\n",
    "    for i in range(0,len(new_HA)):\n",
    "      tree = new_HA[i]\n",
    "      for subtree in tree:\n",
    "        if type(subtree) == nltk.tree.Tree:\n",
    "          if(subtree.label()==\"NP\"):\n",
    "            np=[]\n",
    "            for leaf in subtree:\n",
    "              np.append(leaf[0])\n",
    "            # np=np[:-1]\n",
    "            HA_NP.append(np)\n",
    "    # print(HA_NP)\n",
    "    def intersection(lst1, lst2):\n",
    "        lst3 = [value for value in lst1 if value in lst2]\n",
    "        return lst3\n",
    "    def remove_duplication(lst1):\n",
    "        lst2=[]\n",
    "        for i in lst1:\n",
    "          if(i not in lst2): lst2.append(i)\n",
    "        return lst2\n",
    "    def union_list(lst1,lst2):\n",
    "        lst3 = lst1+lst2\n",
    "        return lst3\n",
    "    CGT = intersection(CRE_NP,HA_NP)\n",
    "    # print(CGT)\n",
    "    new_CRE_tokens1 = []\n",
    "    cgt_set = set()\n",
    "    for i in CGT: \n",
    "      for j in i:\n",
    "        cgt_set.add(j)\n",
    "\n",
    "    for i in new_CRE_tokens:\n",
    "      tmp=[]\n",
    "      for j in i:\n",
    "        if(j in cgt_set):tmp.append(\"_\"+j+\"_\")\n",
    "        else :tmp.append(j)\n",
    "      new_CRE_tokens1.append(tmp)\n",
    "    # print(new_CRE_tokens1)\n",
    "    from gensim.models import Word2Vec\n",
    "    gensim_corpus = union_list( new_CRE_tokens1, new_HA_tokens)\n",
    "    model = Word2Vec(min_count=1, vector_size=100, window=10,sg=1,negative=15)\n",
    "    model.build_vocab(gensim_corpus)\n",
    "    model.train(gensim_corpus, total_examples = model.corpus_count, epochs=model.epochs)\n",
    "    # str=CGT[0]\n",
    "    # model.wv.most_similar(str)\n",
    "    final_CGT = []\n",
    "    for i in CGT:\n",
    "      # sum=model.wv.similarity(i,\"_\"+i+\"_\")\n",
    "      # if(sum>=0.5):\n",
    "      sum=0.00\n",
    "      cnt=0.00\n",
    "      chk=1\n",
    "      for j in i:\n",
    "        if model.wv.similarity(j,\"_\"+j+\"_\")>=0.5:\n",
    "          chk=0\n",
    "    # sum+=model.wv.similarity(j,\"_\"+j+\"_\")\n",
    "    # cnt+=1\n",
    "      if chk==0:\n",
    "        final_CGT.append(i)\n",
    "    # print(final_CGT)\n",
    "    final_result = []\n",
    "    for i in final_CGT:\n",
    "      str=\"\"\n",
    "      for j in i:\n",
    "        str+=j+\" \"\n",
    "      str = str[:-1]\n",
    "      final_result.append(str)\n",
    "    final_result = remove_duplication(final_result)\n",
    "    # print(len(final_result))\n",
    "    final_result.sort()\n",
    "    df = pd.DataFrame(final_result)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request,make_response\n",
    "import io\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef171875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "# from flask.ext import excel\n",
    "app=Flask(__name__)\n",
    "# run_with_ngrok(app)\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "  return render_template('index.html')\n",
    "@app.route(\"/ml\",methods=['POST','GET'])\n",
    "def test():\n",
    "  if request.method=='POST':\n",
    "    csvFile=request.files['csv']\n",
    "    df=pd.read_csv(csvFile)\n",
    "#     df=pd.DataFrame(csvFile)\n",
    "#     print(df.columns)\n",
    "    ans=func(df)\n",
    "    data=ans.to_csv()\n",
    "    response = make_response(data)\n",
    "    cd = 'attachment; filename=mycsv.csv'\n",
    "    response.headers['Content-Disposition'] = cd \n",
    "    response.mimetype='text/csv'\n",
    "    return response\n",
    "    \n",
    "#     print(data)\n",
    "#     return data\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b302cdd1e032ee910f5c889c3360c28564c92ad4f326fc3102e39fbe47faee66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
